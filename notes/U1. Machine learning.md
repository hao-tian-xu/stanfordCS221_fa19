*Don't manually code it up, learn it from examples...*

# <u>**L1. Overview**</u>

### A melting pot

- Bayes rule (Bayes, 1763) from **probability**
- Least squares regression (Gauss, 1795) from **astronomy**
- First-order logic (Frege, 1893) from **logic**
- Maximum likelihood (Fisher, 1922) from **statistics**
- Artificial neural networks (McCulloch/Pitts, 1943) from **neuroscience**
- Minimax games (von Neumann, 1944) from **economics**
- Stochastic gradient descent (Robbins/Monro, 1951) from **optimization**
- Uniform cost search (Dijkstra, 1956) from **algorithms**
- Value iteration (Bellman, 1957) from **control theory**

### Two views of AI

- AI agents: how can we create intelligence?
- AI tools: how can we benefit society?

### Are we there yet?

- Machines: narrow tasks, millions of examples
- Humans: diverse tasks, very few examples

### Issues

- Security (adversarial examples)
- Bias in machine translation
- Fairness in criminal risk assessment

## **Paradigm**

- **Modeling**
  - **Inference**
  - **Learning**

## **Machine Learning**

- The main driver of recent successes in AI
- Move from "code" to "data" to manage the information complexity
- Requires a leap of faith: **generalization**

### Reflex

- Examples: linear classifiers, deep neural networks
- Most common models in machine learning
- Fully feed-forward (no backtracking)

### States

- Applications
  - Games: Chess, Go, Pac-Man, Starcraft, etc.
  - Robotics: motion planning
  - Natural language generation: machine translation, image captioning
- State-based models
  - Search problems: you control everything
  - Markov decision processes: against nature (e.g., Blackjack)
  - Adversarial games: against opponent (e.g., chess)

### Variables

- Constraint satisfaction problems: hard constraints (e.g., Sudoku, scheduling)
- Bayesian networks: soft dependencies (e.g., tracking cars from sensors)

### Logic

- Need to
  - Digest **heterogenous** information
  - Reason **deeply** with that information

## **Optimization**

- Discrete optimization: find the best discrete object
  $$
  \min _{p \in \text { Paths }} \operatorname{Cost}(p)
  $$
  - Algorithmic tool: dynamic programming
- Continuous optimization: find the best vector of real numbers
  $$
  \min _{\mathbf{w} \in \mathbb{R}^{d}} \operatorname {TrainingError}(\mathbf{w})
  $$
  - Algorithmic tool: gradient descent

### Dynamic Programming

- Recurrence + Memoization

### Gradient Descent

$$
w \leftarrow w-\eta F^{\prime}(w) \\
\text{where, } \eta>0 \text{ is a step size}
$$

## *Thinking*

- The history and overview of artificial intelligence

  - We are still far from creating "intelligence", but have more capable tools to serve society
- Optimization: discrete - dynamic programming, continuous - gradient descent



- 介绍了人工智能的历史和概况

  - 我们离创造“智慧”还很远，但拥有更强能力的服务社会的工具
  - 最优化：离散--动态规划、连续--梯度下降

# <u>**L2. Machine learning 1 - Linear Classifiers, SGD**</u>

### Framework: Learning as Optimization

<img src="image.assets/Screen Shot 2021-12-24 at 17.31.59.png" alt="Screen Shot 2021-12-24 at 17.31.59" style="zoom: 25%;" />

## **linear predictors**

### Types of prediction tasks

- Binary classification (e.g., email spam/not spam)
- Regression (e.g., location, year housing price)
- Multiclass classification: $y$ is a category
- Ranking: $y$ is a permutation
- Structured prediction: $y$ is an object which is built from parts

### Feature vector

- For an input $x$, its feature vector is:
  $$
  \phi(x)=\left[\phi_{1}(x), \ldots, \phi_{d}(x)\right] .
  $$
  Think of $\phi(x) \in \mathbb{R}^{d}$ as a point in a high-dimensional space.

### Weight vector

- Weight vector: for each feature $j$, have real number $w_{j}$ representing contribution of feature to prediction

### Linear predictors

- Score: weighted combination of features
  $$
  \mathbf{w} \cdot \phi(x)=\sum_{j=1}^{d} w_{j} \phi(x)_{j}
  $$

- (binary) linear classifier
  $$
  f_{\mathbf{w}}(x)=\operatorname{sign}(\mathbf{w} \cdot \phi(x))= \begin{cases}+1 & \text { if } \mathbf{w} \cdot \phi(x)>0 \\ -1 & \text { if } \mathbf{w} \cdot \phi(x)<0 \\ ? & \text { if } \mathbf{w} \cdot \phi(x)=0\end{cases}
  $$

### Geometric intuition

- A binary classifier $f_{\mathbf{w}}$ defines a hyperplane with normal vector $\mathbf{w}$.
  $\left(\mathbb{R}^{2} \Longrightarrow\right.$ hyperplane is a line; $\mathbb{R}^{3} \Longrightarrow$ hyperplane is a plane)

## **loss minimization**

- <u>Loss functions</u>: $\operatorname{Loss}(x, y, \mathbf{w})$ quantifies how unhappy you would be if you used $\mathbf{w}$ to make a prediction on $x$ when the correct output is $y$. 
  It is the object we want to minimize.
- <u>Score</u>: The score on an example $(x, y)$ is $\mathbf{w} \cdot \phi(x)$, how confident we are in predicting $+1$.
- <u>Margin</u>: The margin on an example $(x, y)$ is $(\mathbf{w} \cdot \phi(x)) y$, how correct we are.
- <u>Residual</u>: The residual is $(\mathbf{w} \cdot \phi(x))-y$, the amount by which prediction $f_{\mathbf{w}}(x)=\mathbf{w} \cdot \phi(x)$ overshoots the target $y$.

### Zero-one loss

- ​	<img src="image.assets/Screen Shot 2021-12-24 at 17.36.00.png" alt="Screen Shot 2021-12-24 at 17.36.00" style="zoom:25%;" />
  $$
  \begin{aligned}
  \operatorname{Loss}_{0-1}(x, y, \mathbf{w}) &=\mathbf{1}[\underbrace{f_{\mathbf{w}}(x)}_{\text{prediction}} \neq y] \\
  &=\mathbf{1} [\underbrace{(\mathbf{w} \cdot \phi(x)) y}_{\text {margin }} \leq 0]
  \end{aligned}
  $$

### Squared loss

$$
\operatorname{Loss}_{\text {squared }}(x, y, \mathbf{w})=(\underbrace{f_{\mathbf{w}}(x)-y}_{\text {residual }})^{2}
$$

- squared loss: grows exponentially
- absolute deviation loss: not smooth

### Loss minimization framework

$$
\begin{aligned}
\operatorname{TrainLoss}(\mathbf{w}) &=\frac{1}{\left|\mathcal{D}_{\text {train }}\right|} \sum_{(x, y) \in \mathcal{D}_{\text {train }}} \operatorname{Loss}(x, y, \mathbf{w}) \\
& \min _{\mathbf{w} \in \mathbb{R}^{d}} \operatorname{Train} \operatorname{Loss}(\mathbf{w})
\end{aligned}
$$

## **stochastic gradient descent**

### Gradient descent

$$
\begin{aligned}
&\text { Initialize } \mathbf{w}=[0, \ldots, 0] \\
&\text { For } t=1, \ldots, T: \\
&\qquad \mathbf{w} \leftarrow \mathbf{w}-\underbrace{\eta}_{\text {step size }} \underbrace{\nabla_{\mathbf{w}} \operatorname{Train} \operatorname{Loss}(\mathbf{w})}_{\text {gradient }}
\end{aligned}
$$

- Objective function:
  $$
  \operatorname{TrainLoss}(\mathbf{w})=\frac{1}{\left|\mathcal{D}_{\text {train }}\right|} \sum_{(x, y) \in \mathcal{D}_{\text {train }}}(\mathbf{w} \cdot \phi(x)-y)^{2}
  $$
- Gradient (use chain rule):
  $$
  \nabla_{\mathbf{w}} \operatorname{Train} \operatorname{Loss}(\mathbf{w})=\frac{1}{\left|\mathcal{D}_{\text {train }}\right|} \sum_{(x, y) \in \mathcal{D}_{\text {train }}} 2(\underbrace{\mathbf{w} \cdot \phi(x)-y}_{\text {prediction-target }}) \phi(x)
  $$

### Gradient descent is slow

- Problem: each iteration requires going over all training examples — expensive when have lots of data!

### Stochastic gradient descent

- Gradient descent (GD):
  $\mathbf{w} \leftarrow \mathbf{w}-\eta \nabla_{\mathbf{w}}$ TrainLoss $(\mathbf{w})$
- Stochastic gradient descent (SGD):
  For each $(x, y) \in \mathcal{D}_{\text {train }}$ :
  $\mathbf{w} \leftarrow \mathbf{w}-\eta \nabla_{\mathbf{w}} \operatorname{Loss}(x, y, \mathbf{w})$
- It’s not about **quality**, it’s about **quantity**

### Step size

- Strategies:
  - Constant: $\eta=0.1$
  - Decreasing: $\eta=1 / \sqrt{\# \text { updates made so far }}$

### What about classification?

- Problems:
  - Gradient of Loss $_{0-1}$ is 0 everywhere, SGD not applicable
  - Loss $_{0-1}$ is insensitive to how badly the model messed up
- Hinge loss (SVMs)
  <img src="image.assets/Screen Shot 2021-12-24 at 18.03.30.png" alt="Screen Shot 2021-12-24 at 18.03.30" style="zoom:25%;" />
  $$
  \operatorname { Loss_{hinge } }(x, y, \mathbf{w})=\max \{1-(\mathbf{w} \cdot \phi(x)) y, 0\}
  $$
  - Intuition: hinge loss upper bounds 0-1 loss, has non-trivial gradient
  - Try to increase margin if it is less than 1
- Logistic regression
  <img src="image.assets/Screen Shot 2021-12-24 at 18.05.01.png" alt="Screen Shot 2021-12-24 at 18.05.01" style="zoom:25%;" />
	$$
  \operatorname{Loss}_{\text {logistic }}(x, y, \mathbf{w})=\log \left(1+e^{-(\mathbf{w} \cdot \phi(x)) y}\right)
  $$
  - Intuition: Try to increase margin even when it already exceeds 1
  

## **Summary**

- Linear predictors:
	$$
  f_{\mathbf{w}}(x) \text { based on score } \mathbf{w} \cdot \phi(x)
  $$
- Loss minimization: learning as optimization
  $$
  \min _{\mathbf{w}} \text { TrainLoss }(\mathbf{w})
  $$
- Stochastic gradient descent: optimization algorithm
  $$
  \mathbf{w} \leftarrow \mathbf{w}-\eta \nabla_{\mathbf{w}} \operatorname{Loss}(x, y, \mathbf{w})
  $$

|                          | Classification                  | Linear regression              |
| ------------------------ | ------------------------------- | ------------------------------ |
| Predictor $f_\mathbf{w}$ | sign (score)                    | score                          |
| Relate to correct $y$    | margin (score $y$ )             | residual (score - $y$ )        |
| Loss functions           | zero-one<br/>hinge<br/>logistic | squared<br/>absolute deviation |
| Algorithm                | SGD                             | SGD                            |

## *Thinking*

- Learning as Optimization
- Introduces the basic concepts of machine learning, and algorithms for two basic problems (binary classification and linear regression)
  1. the goal is to learn a weight vector (as a parameter)
  2. the score is the dot product of the feature vector and the weight vector
  3. the predictor produces a value related to the actual result by processing the score
  4. the loss function calculates the deviation between the predicted and actual results
  5. optimal weight vector is derived by gradient descent
     - Stochastic gradient descent increases the speed



- 学习即优化
- 介绍了机器学习的基础概念，两种基本问题（二元分类和线性回归）的算法
  1. 目标是学习一个权重向量（作为参数）
  2. 分数为特征向量和权重向量的点积
  3. 预测器通过对分数的处理得出和实际结果相关的值
  4. 损失函数计算预测和实际结果间的偏差
  5. 通过梯度下降得出最优权重向量
     - 随机梯度下降提高速度

# <u>**L3. Machine Learning 2 - Features, Neural Networks**</u>

## **features**

### Feature extraction

- Feature template
  - A feature template is a group of features all computed in a similar way.
  - usually sparse
- Feature vector representations: 
  - Array representation (good for dense features)
  - Map representation (good for sparse features)
- Hypothesis class
  - A hypothesis class is the set of possible predictors with a fixed $\phi(x)$ and varying $\mathbf{w}$ :
    $\mathcal{F}=\left\{f_{\mathbf{w}}: \mathbf{w} \in \mathbb{R}^{d}\right\}$
  - e.g. Predictors $f_{\mathrm{w}}(x)=\mathrm{w} \cdot \phi(x) \quad$ or $\quad \operatorname{sign}(\mathrm{w} \cdot \phi(x))$    with varying $\mathrm{w}$

### Feature extraction + learning

- Feature extraction: set $\mathcal{F}$ based on domain knowledge
- Learning: set $f_{\mathbf{w}} \in \mathcal{F}$ based on data

### linear in what?

- Prediction driven by score:
  $$
  \mathbf{w} \cdot \phi(x)
  $$
  Linear in w?         Yes
  Linear in $\phi(x)$ ？ Yes
  Linear in $x$ ?        No!
  ( $x$ not necessarily even a vector)

## **neural networks**

- Decomposing the problem
- Logistic function
  - The logistic function maps $(-\infty, \infty)$ to $[0,1]$ : 
    $\sigma(z)=\left(1+e^{-z}\right)^{-1}$
  - Derivative:
    $\sigma^{\prime}(z)=\sigma(z)(1-\sigma(z))$

### Linear functions vs. Neural networds

- Linear functions
  <img src="image.assets/Screen Shot 2021-12-27 at 15.59.30.png" alt="Screen Shot 2021-12-27 at 15.59.30" style="zoom:25%;" />
  - Output
  	$$
    \text { score }=\mathbf{w} \cdot \phi(x)
    $$

- Neural networks
  <img src="image.assets/Screen Shot 2021-12-27 at 16.00.16.png" alt="Screen Shot 2021-12-27 at 16.00.16" style="zoom:25%;" />
  - Intermediate hidden units:
    $$
    h_{j}=\sigma\left(\mathbf{v}_{j} \cdot \phi(x)\right) \quad \sigma(z)=\left(1+e^{-z}\right)^{-1}
    $$
  - Output:
    $$
    \text { score }=\mathbf{w} \cdot \mathbf{h}
    $$

### Learned features

- intermediate hidden units as learned features of a linear predictor
  - Before: apply linear predictor on manually specify features 
    $$
    \phi(x)
    $$
  - Now: apply linear predictor on automatically learned features 
    $$
    h(x)=\left[h_{1}(x), \ldots, h_{k}(x)\right]
    $$
    
## **gradient without tears**

### Computation graph

- Advantages: 
  - Avoid long equations 
  - Reveal structure of computations (modularity, efficiency, dependencies) — TensorFlow/PyTorch are built on this
- Basic building blocks
  - plus, minus, product, max, logistic

### Backpropagation

- Definition
  - Forward: $f_{i}$ is value for subexpression rooted at $i$ 
  - Backward: $g_{i}=\frac{\partial \text { out }}{\partial f_{i}}$ is how $f_{i}$ influences output
- Algorithm
  - Forward pass: compute each $f_{i}$ (from leaves to root)
  - Backward pass: compute each $g_{i}$ (from root to leaves)

### Note on optimization

<img src="image.assets/Screen Shot 2021-12-27 at 16.15.31.png" alt="Screen Shot 2021-12-27 at 16.15.31" style="zoom:25%;" />

## **neareast neighbors**

<img src="image.assets/Screen Shot 2021-12-27 at 16.21.39.png" alt="Screen Shot 2021-12-27 at 16.21.39" style="zoom:25%;" />

- Decision boundary: based on Voronoi diagram
- Algoritm
  - Training: just store $\mathcal{D}_{\text {train }}$
  - Predictor $f\left(x^{\prime}\right)$ :
    - Find $(x, y) \in \mathcal{D}_{\text {train }}$ where $\left\|\phi(x)-\phi\left(x^{\prime}\right)\right\|$ is smallest
    - Return $y$
- Expressivity
  - Much more expressive than quadratic features 
  - Non-parametric: the hypothesis class adapts to number of examples 
  - Simple and powerful, but kind of brute force

## **Summary of learners**

- Linear predictors: combine raw features 
  - prediction is fast, easy to learn, **weak** use of features 
- Neural networks: combine learned features 
  - prediction is fast, **hard** to learn, powerful use of features 
- Nearest neighbors: predict according to similar examples 
  - prediction is **slow**, easy to learn, powerful use of features

## *Thinking*

- Methods of feature extraction and introduction of some concepts
  - Feature templates, hypothesis classes, etc.
- Neural networks: the motivation is to set up hidden units to automate feature extraction
- Computational graphs: motivated by automating the acquisition of differential equations
  - Backpropagation
- Nearest neighbor algorithm: very expressive, but slow prediction speed (almost brute force search)
- Comparison of each predictor, in fact each has its own advantages and disadvantages



- 特征提取的方法以及一些概念的介绍
  - 特征模板，假设类等
- 神经网络：动机在于设置隐藏单元将特征提取自动化
- 计算图：动机在于将微分方程的获得自动化
  - 反向传播：等section详细介绍
- 近邻算法：表达能力很强，但是预测速度慢（几乎是暴力搜索）
- 各预测器的比较，事实上各有各的优势和缺点

# **<u>L4. Machine Learning 3 - Generalization, K-means</u>**

## **generalization**

- objective: Our goal is to minimize error on <u>unseen future examples</u>.

### Overfitting

<img src="image.assets/Screen Shot 2021-12-28 at 09.47.40.png" alt="Screen Shot 2021-12-28 at 09.47.40" style="zoom:25%;" />

- Test set: $\mathcal{D}_{\text {test }}$ contains examples not used for training.

### Approximation and estimation error

<img src="image.assets/Screen Shot 2021-12-28 at 09.50.42.png" alt="Screen Shot 2021-12-28 at 09.50.42" style="zoom:25%;" />

- <u>Approximation error</u>: how good is the hypothesis class? 
- <u>Estimation error</u>: how good is the learned predictor relative to the potential of the hypothesis class?
- As the hypothesis class size increases...
  - Approximation error decreases because: taking min over larger set
  - Estimation error increases because: harder to estimate something more complex

### Strategy 1: dimensionality

- Controlling the dimensionality
  - Manual feature (template) selection:
    - Add features if they help
    - Remove features if they don’t help
  - Automatic feature selection (beyond the scope of this class):
    - Forward selection
    - Boosting
    - $L_1$ regularization

### Strategy 2: norm

- Regularized objective:
  $$
  \min _{\mathbf{w}} \operatorname{Train} \operatorname{Loss}(\mathbf{w})+\frac{\lambda}{2}\|\mathbf{w}\|^{2}
  $$

  - control the scale of $\mathbf{w}$
  - Occam's Razor: if simple weights can work, complex ones are not neccessary

- Early stopping

  - Idea: simply make $T$ smaller
  - Intuition: if have fewer updates, then $\|\mathbf{w}\|$ can't get too big.
  - Lesson: try to minimize the training error, but don't try too hard.


### Summary

- Try to minimize training error, but keep the hypothesis class small.

### Hyperparameters

- Choose hyperparameters to minimize $\mathcal{D}_{\text {train }}$ error? **No** - solution would be to include all features, set $\lambda=0, T \rightarrow \infty$.
- Choose hyperparameters to minimize $\mathcal{D}_{\text {test }}$ error? **No** - choosing based on $\mathcal{D}_{\text {test }}$ makes it an unreliable estimate of error!
- Validation
  - Problem: can't use test set!
  - Solution: randomly take out 10-50\% of training data and use it instead of the test set to estimate test error.

### Development cycle

- Split data into train, val, test 
- Look at data to get intuition 
- Repeat: 
  - Implement feature / tune hyperparameters 
  - Run learning algorithm 
  - Sanity check train and val error rates, weights 
  - Look at errors to brainstorm improvements 
- Run on test set to get final error rates

## **unsupervised learning**

### Supervision

- Supervised learning:
  - Prediction: $\mathcal{D}_{\text {train }}$ contains input-output pairs $(x, y)$
  - Fully-labeled data is very expensive to obtain (we can maybe get thousands of labeled examples)
- Unsupervised learning:
  - Clustering: $\mathcal{D}_{\text {train }}$ only contains inputs $x$
  - Unlabeled data is much cheaper to obtain (we can maybe get billions of unlabeled examples)

### Types of unsupervised learning

- Clustering (e.g., K-means)
- Dimensionality reduction (e.g., PCA)

### Clustering

- Input: training set of input points
  $$
  \mathcal{D}_{\text {train }}=\left\{x_{1}, \ldots, x_{n}\right\}
  $$
- Output: assignment of each point to a cluster
	$$
  \left[z_{1}, \ldots, z_{n}\right] \text { where } z_{i} \in\{1, \ldots, K\}
  $$

### K-means objective

- Setup:
  - Each cluster $k=1, \ldots, K$ is represented by a centroid $\mu_{k} \in \mathbb{R}^{d}$
  - Intuition: want each point $\phi\left(x_{i}\right)$ close to its assigned centroid $\mu_{z_{i}}$
- Objective function:
  $$
  \operatorname{Loss}_{\mathrm{kmeans}}(z, \mu)=\sum_{i=1}^{n}\left\|\phi\left(x_{i}\right)-\mu_{z_{i}}\right\|^{2}
  $$

### K-means algorithm

- Step 1

  - For each point $i=1, \ldots, n$ :
    Assign $i$ to cluster with closest centroid:
    $$
    z_{i} \leftarrow \arg \min _{k=1, \ldots, K}\left\|\phi\left(x_{i}\right)-\mu_{k}\right\|^{2} .
    $$

- Step 2

  - For each cluster $k=1, \ldots, K$ :
    Set $\mu_{k}$ to average of points assigned to cluster $k$ :
    $$
    \mu_{k} \leftarrow \frac{1}{\left|\left\{i: z_{i}=k\right\}\right|} \sum_{i: z_{i}=k} \phi\left(x_{i}\right)
    $$

- Algorithm

  - Initialize $\mu_{1}, \ldots, \mu_{K}$ randomly.
    For $t=1, \ldots, T$ :
    	Step 1: set assignments $z$ given $\mu$
    	Step 2: set centroids $\mu$ given $z$

### Local minima

- K-means is guaranteed to converge to a local minimum, but is not guaranteed to find the global minimum
- Solutions:
  - Run multiple times from different random initializations
  - • Initialize with a heuristic (K-means++)

## **Summary**

- Feature extraction (think hypothesis classes) [modeling] 
- Prediction (linear, neural network, k-means) [modeling] 
- Loss functions (compute gradients) [modeling] 
- Optimization (stochastic gradient, alternating minimization) [learning] 
- Generalization (think development cycle) [modeling]

### A brief history

- 1795: Gauss proposed least squares (astronomy) 
- 1940s: logistic regression (statistics) 
- 1952: Arthur Samuel built program that learned to play checkers (AI) 
- 1957: Rosenblatt invented Perceptron algorithm (like SGD) 
- 1969: Minsky and Papert ”killed” machine learning 
- 1980s: neural networks (backpropagation, from 1960s) 
- 1990: interface with optimization/statistics, SVMs 
- 2000s-: structured prediction, revival of neural networks, etc.

## *Thinking*

- Generalization / Overfitting solutions
  - Control dimensionality: remove useless feature vectors
  - Control norm:
    - Control the scale of the weight vector - if a small weight vector can be solved, don't use a large one
    - Control the number of training iterations
- Unsupervised learning
  - Clustering: treats samples as points in space and groups them
    - k-means: randomly initialize centers, group and calculate centers for each group reciprocally



- 泛化 / 过拟合的解决方案
  - 控制维度：去除无用的特征向量
  - 控制norm：
    - 控制权重向量的尺度——如果小的权重向量可以解决，就不用大的
    - 控制训练次数
- 无监督学习
  - 聚类：将样本看作空间中的点，将其分组
    - k均值：随机初始化中心，分组和计算各组的中心往复进行









 
