# <u>**L7. Policy evaluation, policy improvement, Policy iteration, value iteration**</u>

## **Markov decision process**

### e.g. MDP for dice game

- For each round $r=1,2, \ldots$
  - You choose stay or quit.
  - If quit, you get $\$ 10$ and we end the game.
  - If stay, you get $\$ 4$ and then I roll a 6-sided dice.
    - If the dice results in 1 or 2 , we end the game.
    - Otherwise, continue to the next round.

<img src="image.assets/Screen Shot 2022-01-06 at 11.06.31.png" alt="Screen Shot 2022-01-06 at 11.06.31" style="zoom:25%;" />

### Markov decision process

- Definition
  - States: the set of states
  - $s_{\text {start }} \in$ States: starting state
  - $\operatorname{Actions} (s)$ : possible actions from state $s$
  - $T\left(s, a, s^{\prime}\right)$ : probability of $s^{\prime}$ if take action $a$ in state $s$
  - $\operatorname{Reward}\left(s, a, s^{\prime}\right)$ : reward for the transition $\left(s, a, s^{\prime}\right)$
  - $\operatorname{IsEnd}(s)$ : whether at end of game
  - $0 \leq \gamma \leq 1$ : discount factor (default: 1)
- Difference from Search problem
  -  We can think of the successor function $\operatorname{Succ}(s, a)$ as a special case of transition probabilities:
    - $T\left(s, a, s^{\prime}\right)= \begin{cases}1 & \text { if } s^{\prime}=\operatorname{Succ}(s, a) \\ 0 & \text { otherwise }\end{cases}$
  - A minor difference is that we’ve gone from minimizing costs to maximizing rewards. The two are really equivalent: you can negate one to get the other.
- Transition probabilities
  - Definition: The transition probabilities $T\left(s, a, s^{\prime}\right)$ specify the probability of ending up in state $s^{\prime}$ if taken action $a$ in state $s$.
  - Probabilities sum to one:
    - For each state $s$ and action $a$ : $\sum_{s^{\prime} \in \text { States }} T\left(s, a, s^{\prime}\right)=1$

### What is a solution?

- Definition: policy
  - A policy $\pi$ is a mapping from each state $s \in$ States to an action $a \in \operatorname{Actions}(s)$.

## **Policy evaluation**

- Definition: utility
  - Following a policy yields a random path. The utility of a policy is the (discounted) sum of the rewards on the path (this is a random quantity).
- Definition: value (expected utility) 
  - The value of a policy is the expected utility.
- Definition: discount
  - Path: $s_{0}, a_{1} r_{1} s_{1}, a_{2} r_{2} s_{2}, \ldots$ (action, reward, new state).
    The utility with discount $\gamma$ is $u_{1}=r_{1}+\gamma r_{2}+\gamma^{2} r_{3}+\gamma^{3} r_{4}+\cdots$

### Policy evaluation

- Definition: value of a policy
  - Let $V_{\pi}(s)$ be the expected utility received by following policy $\pi$ from state $s$.
- Definition: Q-value of a policy
  - Let $Q_{\pi}(s, a)$ be the expected utility of taking action $a$ from state $s$, and then following policy $\pi$.

$$
\begin{aligned}
&V_{\pi}(s)= \begin{cases}0 & \text { if IsEnd }(s) \\
Q_{\pi}(s, \pi(s)) & \text { otherwise. }\end{cases} \\
\\
&Q_{\pi}(s, a)=\sum_{s^{\prime}} T\left(s, a, s^{\prime}\right)\left[\text { Reward }\left(s, a, s^{\prime}\right)+\gamma V_{\pi}\left(s^{\prime}\right)\right]
\end{aligned}
$$

### e.g. Dice game

- Let $\pi$ be the "stay" policy: $\pi($ in $)=$ stay.
  - $V_{\pi}(\text { end })=0$
  - $V_{\pi}(\text { in })=\frac{1}{3}\left(4+V_{\pi}(\text { end })\right)+\frac{2}{3}\left(4+V_{\pi}(\text { in })\right)$
- In this case, can solve in closed form:
  - $V_{\pi} \text { (in) }=12$

### Iterative algorithm

- Start with arbitrary policy values and repeatedly apply recurrences to converge to true values.
  - Initialize $V_{\pi}^{(0)}(s) \leftarrow 0$ for all states $s$.
  - For iteration $t=1, \ldots, t_{\mathrm{PE}}$ :
    - For each state $s$ :
      $$
      V_{\pi}^{(t)}(s) \leftarrow \underbrace{\sum_{s^{\prime}} T\left(s, \pi(s), s^{\prime}\right)\left[\operatorname{Reward}\left(s, \pi(s), s^{\prime}\right)+\gamma V_{\pi}^{(t-1)}\left(s^{\prime}\right)\right]}_{Q^{(t-1)}(s, \pi(s))}
      $$
    - How many iterations (t~PE~)? Repeat until values don’t change much:
    	$$
      \max _{s \in \text { States }}\left|V_{\pi}^{(t)}(s)-V_{\pi}^{(t-1)}(s)\right| \leq \epsilon
      $$
  
- Time Complexity: $O\left(t_{\mathrm{PE}} S S^{\prime}\right)$

## **Value iteration**

### Optimal value

- Definition

  - The optimal value $V_{\mathrm{opt}}(s)$ is the maximum value attained by any policy.


<img src="image.assets/Screen Shot 2022-01-06 at 11.48.58.png" alt="Screen Shot 2022-01-06 at 11.48.58" style="zoom:25%;" />

- Optimal value if take action $a$ in state $s$ :
  $$
  Q_{\mathrm{opt}}(s, a)=\sum_{s^{\prime}} T\left(s, a, s^{\prime}\right)\left[\operatorname{Reward}\left(s, a, s^{\prime}\right)+\gamma V_{\mathrm{opt}}\left(s^{\prime}\right)\right] .
  $$

- Optimal value from state $s$ :
  $$
  V_{\mathrm{opt}}(s)= \begin{cases}0 & \text { if IsEnd }(s) \\ \max _{a \in \operatorname{Actions}(s)} Q_{\mathrm{opt}}(s, a) & \text { otherwise. }\end{cases}
  $$

### Optimal policies

- Given $Q_{\text {opt }}$, read off the optimal policy:
  $$
  \pi_{\mathrm{opt}}(s)=\arg \max _{a \in \operatorname{Actions}(s)} Q_{\mathrm{opt}}(s, a)
  $$

### Value iteration

- Initialize $V_{\text {opt }}^{(0)}(s) \leftarrow 0$ for all states $s$.

- For iteration $t=1, \ldots, t_{\mathrm{VI}}$ :
  - For each state $s$ :
    $$
    V_{\text {opt }}^{(t)}(s) \leftarrow \max _{a \in \text { Actions }(s)} \underbrace{\sum_{s^{\prime}} T\left(s, a, s^{\prime}\right)\left[\operatorname{Reward}\left(s, a, s^{\prime}\right)+\gamma V_{\text {opt }}^{(t-1)}\left(s^{\prime}\right)\right]}_{Q_{\text {opt }}^{(t-1)}(s, a)}
    $$

- Time Complexity: $O\left(t_{\mathrm{VI}} S A S^{\prime}\right)$

### Convergence

- Suppose either
  - discount $\gamma<1$, or
  - MDP graph is acyclic.
- Then value iteration converges to the correct answer.

## **Summary**

- MDP: graph with states, chance nodes, transition probabilities, rewards
- Policy: mapping from state to action (solution to MDP)
- Value of policy: expected utility over random paths
- Policy evaluation: iterative algorithm to compute value of policy
  - $(\text{MDP}, \pi)\rightarrow V_{\pi}$

- Value iteration: computes optimal value (maximum expected utility) and optimal policy
  - $\text{MDP}\rightarrow\left(V_{\mathrm{opt}}, \pi_{\mathrm{opt}}\right)$

- Main technique
  - Write down recurrence (e.g., $V_{\pi}(s)=\cdots V_{\pi}\left(s^{\prime}\right) \cdots$ )
  - Turn into iterative algorithm (replace mathematical equality with assignment operator)

## *Thinking*

- 马尔可夫决策过程，学习自动机
  - 和搜索的差别主要在于行动导致的结果<u>不再是决定性的</u>
  - 因此问题的结果不再是一个路径，而是一个对应状态的决策策略policy
  - 获得该策略需要知道每个状态的最大预期效用：数值迭代算法
    - 若已知策略，则通过策略评价算法获得该策略的价值
- 核心算法设计思想：
  - 将递归公式转化为迭代算法，从随机初始价值开始通过多世代的迭代从而收敛

# **<u>L8. Reinforcement learning, Monte Carlo, SARSA, Q-learning, Exploration/exploitation, function approximation</u>**

## **Reinforcement learning**

### From MDPs to reinforcement learning

- Markov decision process (offline) 
  - Have mental model of how the world works. 
  - Find policy to collect maximum rewards. 
- Reinforcement learning (online)
  - Don’t know how the world works.
  - Perform actions in the world to find out and collect rewards.

### Reinforcement learning framework

<img src="image.assets/Screen Shot 2022-01-06 at 13.58.15.png" alt="Screen Shot 2022-01-06 at 13.58.15" style="zoom:25%;" />

## **Monte Carlo methods**

### Model-based Monte Carlo

- Key idea: model-based learning
  - Estimate the MDP: $T\left(s, a, s^{\prime}\right)$ and $\operatorname{Reward}\left(s, a, s^{\prime}\right)$
  - Transitions:
    $$
    \hat{T}\left(s, a, s^{\prime}\right)=\frac{\# \text { times }\left(s, a, s^{\prime}\right) \text { occurs }}{\# \text { times }(s, a) \text { occurs }}
    $$
  - Rewards:
    $$
    \widehat{\text { Reward }}\left(s, a, s^{\prime}\right)=r \text { in }\left(s, a, r, s^{\prime}\right)
    $$
- Problem: won't even see $(s, a)$ if $a \neq \pi(s)$

  - Key idea: exploration 
    - To do reinforcement learning, need to explore the state space.

  - Solution: need $\pi$ to explore explicitly (more on this later)

### Model-free Monte Carlo

- Key idea: model-free learning
  - All that matters for prediction is (estimate of) $Q_{\mathrm{opt}}(s, a)$
  - Data (following policy $\pi$ ):
    - $s_{0} ; a_{1}, r_{1}, s_{1} ; a_{2}, r_{2}, s_{2} ; a_{3}, r_{3}, s_{3} ; \ldots ; a_{n}, r_{n}, s_{n}$
  - Recall:
    - $Q_{\pi}(s, a)$ is expected utility starting at $s$, first taking action $a$, and then following policy $\pi$
  - Utility:
    - $u_{t}=r_{t}+\gamma \cdot r_{t+1}+\gamma^{2} \cdot r_{t+2}+\cdots$
  - Estimate:
  	$$
    \begin{aligned}
    &\hat{Q}_{\pi}(s, a)=\text { average of } u_{t} \text { where } s_{t-1}=s, a_{t}=a \\
    &\text { (and } s, a \text { doesn't occur in } s_{0}, \cdots, s_{t-2} \text { ) }
    \end{aligned}
    $$
- Equivalent formulation (convex combination)
  - On each $(s, a, u):$
    $$
    \begin{aligned}
    &\eta=\frac{1}{1+(\# \text { updates to }(s, a))} \\
    &\hat{Q}_{\pi}(s, a) \leftarrow(1-\eta) \hat{Q}_{\pi}(s, a)+\eta u
    \end{aligned}
    $$
- Equivalent formulation (stochastic gradient)
  - On each $(s, a, u)$ :
    $$
    \hat{Q}_{\pi}(s, a) \leftarrow \hat{Q}_{\pi}(s, a)-\eta[\underbrace{\hat{Q}_{\pi}(s, a)}_{\text {prediction }}-\underbrace{u}_{\text {target }}]
    $$
    Implied objective: least squares regression $\left(\hat{Q}_{\pi}(s, a)-u\right)^{2}$

## **Bootstrapping methods**

### SARSA

- Broadly speaking, reinforcement learning algorithms interpolate between new data (which specifies the **target** value) and the old estimate of the value (the **prediction**).

- Problem: If the episode is long, target $u$ will be a pretty lousy estimate.

- Solution: Bootstrapping

  - SARSA uses estimate $\hat{Q}_{\pi}(s, a)$ instead of just raw data $u$.

  - On each $\left(s, a, r, s^{\prime}, a^{\prime}\right)$ :
    $$
    \hat{Q}_{\pi}(s, a) \leftarrow(1-\eta) \hat{Q}_{\pi}(s, a)+\eta[\underbrace{r}_{\text {data }}+\gamma \underbrace{\hat{Q}_{\pi}\left(s^{\prime}, a^{\prime}\right)}_{\text {estimate }}]
    $$

| target | $u$                      | $r+\hat{Q}_{\pi}\left(s^{\prime}, a^{\prime}\right)$ |
| ------ | ------------------------ | ---------------------------------------------------- |
|        | based on one path        | based on estimate                                    |
|        | unbiased                 | biased                                               |
|        | large variance           | small variance                                       |
|        | wait until end to update | can update immediately                               |

### Q-learning

- Problem: model-free Monte Carlo and SARSA only estimate $Q_{\pi}$, but want $Q_{\mathrm{opt}}$ to act optimally
- Solution: Q-learning
  - On each $\left(s, a, r, s^{\prime}\right):$
    $$
    \hat{Q}_{\text {opt }}(s, a) \leftarrow(1-\eta) \underbrace{\hat{Q}_{\text {opt }}(s, a)}_{\text {prediction }}+\eta \underbrace{\left(r+\gamma \hat{V}_{\text {opt }}\left(s^{\prime}\right)\right)}_{\text {target }}
    $$
    Recall: $\hat{V}_{\mathrm{opt}}\left(s^{\prime}\right)=\max _{a^{\prime} \in \operatorname{Actions}\left(s^{\prime}\right)} \hat{Q}_{\mathrm{opt}}\left(s^{\prime}, a^{\prime}\right)$
    $$
    \left.\hat{Q}_{\mathrm{opt}}(s, a) \leftarrow(1-\eta) \hat{Q}_{\mathrm{opt}}(s, a)+\eta\left(r+\gamma \max _{a^{\prime} \in \operatorname{Actions}\left(s^{\prime}\right)} \hat{Q}_{\mathrm{opt}}\left(s^{\prime}, a^{\prime}\right)\right)\right]
    $$

| Output           | MDP               | reinforcement leraning        | type       |
| ---------------- | ----------------- | ----------------------------- | ---------- |
| $Q_{\pi}$        | policy evaluation | model-free Monte Carlo, SARSA | on-policy  |
| $Q_{\text{opt}}$ | value iteration   | Q-learning                    | off-policy |

## **Covering the unknown**

### Exploration: Epsilon-greedy

- Algorithm: reinforcement learning template
  - For $t=1,2,3, \ldots$
  - Choose action $a_{t}=\pi_{\text {act }}\left(s_{t-1}\right)$ (how?)
  - Receive reward $r_{t}$ and observe new state $s_{t}$
  - Update parameters (how?)
- Which exploration policy $\pi_{\text {act }}$ to use?
  - No exploration, all exploitation
    - Set $\pi_{\text {act }}(s)=\arg \max _{a \in \text { Actions }(s)} \hat{Q}_{\mathrm{opt}}(s, a)$
    - Problem: $\hat{Q}_{\mathrm{opt}}(s, a)$ estimates are inaccurate, too greedy!
  - No exploitation, all exploration
    - Set $\pi_{\mathrm{act}}(s)=\operatorname{random}$ from $\operatorname{Actions}(s)$
    - Problem: average utility is low because exploration is not guided
- Exploration/exploitation tradeof: Epsilon-greedy
	$$
  \pi_{\text {act }}(s)= \begin{cases}\arg \max _{a \in \text { Actions }} \hat{Q}_{\text {opt }}(s, a) & \text { probability } 1-\epsilon \\ \text { random from } \operatorname{Actions}(s) & \text { probability } \epsilon\end{cases}
  $$

### Generalization: Function approximation

- Problem: large state spaces, hard to explore
- Solution: Function approximation
  - Key idea: linear regression model
    - Define features $\phi(s, a)$ and weights w:
      $$
      \hat{Q}_{\mathrm{opt}}(s, a ; \mathbf{w})=\mathbf{w} \cdot \phi(s, a)
      $$
- Q-learning with function approximation
  - On each $\left(s, a, r, s^{\prime}\right)$ :
    $$
    \mathbf{w} \leftarrow \mathbf{w}-\eta[\underbrace{\hat{Q}_{\text {opt }}(s, a ; \mathbf{w})}_{\text {prediction }}-\underbrace{\left(r+\gamma \hat{V}_{\text {opt }}\left(s^{\prime}\right)\right)}_{\text {target }}] \phi(s, a)
    $$
  - Implied objective function: $(\underbrace{\hat{Q}_{\text {opt }}(s, a ; \mathbf{w})}_{\text {prediction }}-\underbrace{\left(r+\gamma \hat{V}_{\text {opt }}\left(s^{\prime}\right)\right)}_{\text {target }})^{2}$

## **Summary**

- Online setting: learn and take actions in the real world!
- Exploration/exploitation tradeoff
- Monte Carlo: estimate transitions, rewards, Q-values from data
- Bootstrapping: update towards target that depends on estimate rather than just raw data

### States and information

|                  | stateless                                         | state                                             |
| ---------------- | ------------------------------------------------- | ------------------------------------------------- |
| full feedback    | supervised learning <br />(binary classification) | supervised learning <br />(structured prediction) |
| partial feedback | multi-armed bandits                               | reinforcement learning                            |

## *Thinking*

- 强化学习：无已知模型，通过数据学习模型的特征
  - 基于模型 vs. 基于Q值（无模型学习）：基于模型的学习指学习MDP的参数，基于Q值则指学习不同（状态，行动）的Q值
  - on-policy vs. off-policy：on-policy指预设中包含指定的策略
  - online vs. offline：online意味着学习过程中实时产生数据进行学习，MDP是offline的
  - 目标：原始数据 vs. 部分数据+预测（无模型蒙特卡罗 vs. SARSA）





