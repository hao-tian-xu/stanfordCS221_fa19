# **<u>L13. Probabilistic inference, Hidden Markov models</u>**

## **Basics**

### Review: probability

- Random variables: sunshine $S \in\{0,1\}$, rain $R \in\{0,1\}$
  - Joint distribution
  - Marginal distribution
  - Conditional distribution
- Probabilistic inference
  - Joint distribution (probabilistic database): $\mathbb{P}(S, R, T, A)$
  - Probabilistic inference:
    - Condition on evidence (traffic, autumn): $T=1, A=1$
    - Interested in query (rain?): $R$
    - $\mathbb{P}(\underbrace{R}_{\text {query }} \mid \underbrace{T=1, A=1}_{\text {condition }})$
      ($S$ is marginalized out)

### Challenges

- Modeling: How to specify a joint distribution $\mathbb{P}\left(X_{1}, \ldots, X_{n}\right)$ compactly?
  - Bayesian networks (factor graphs to specify joint distributions)
- Inference: How to compute queries $\mathbb{P}(R \mid T=1, A=1)$ efficiently?
  - Variable elimination, Gibbs sampling, particle filtering (analogue of algorithms for finding maximum weight assignment)

### Bayesian network (alarm)

- ​	<img src="image.assets/Screen Shot 2022-01-26 at 16.32.30.png" alt="Screen Shot 2022-01-26 at 16.32.30" style="zoom: 25%;" />
- $\mathbb{P}(B=b, E=e, A=a) \stackrel{\text { def }}{=} p(b) p(e) p(a \mid b, e)$
- Bayesian networks are a special case of factor graphs

### Probabilistic inference (alarm)

- Key idea: explaining away
  - Suppose two causes positively influence an effect. Conditioned on the effect, conditioning on one cause reduces the probability of the other cause.
  
- Definition: Bayesian network
  - Let $X=\left(X_{1}, \ldots, X_{n}\right)$ be random variables.
  - A Bayesian network is a directed acyclic graph (DAG) that specifies a joint distribution over $X$ as a product of local conditional distributions, one for each node:
    $$
    \mathbb{P}\left(X_{1}=x_{1}, \ldots, X_{n}=x_{n}\right) \stackrel{\text { def }}{=} \prod_{i=1}^{n} p\left(x_{i} \mid x_{\text {Parents }(i)}\right)
    $$
  
- Special properties

  - Key idea: locally normalized

    - All factors (local conditional distributions) satisfy:
      $$
      \sum_{x_{i}} p\left(x_{i} \mid x_{\text {Parents }(i)}\right)=1 \text { for each } x_{\text {Parents }(i)}
      $$

  - Implications

    - Consistency of sub-Bayesian networks 
    - Consistency of conditional distributions


### Consistency of sub-Bayesian networks

- $\begin{aligned} \mathbb{P}(B=b, E=e) & \stackrel{\text { def }}{=} \sum_{a} \mathbb{P}(B=b, E=e, A=a) \\ & \stackrel{\text { def }}{=} \sum_{a} p(b) p(e) p(a \mid b, e) \\ &=p(b) p(e) \sum_{a} p(a \mid b, e) \\ &=p(b) p(e) \end{aligned}$
- Key idea: marginalization
  - Marginalization of a leaf node yields a Bayesian network without the node.

### Consistency of local conditionals

- Key idea: local conditional distributions
  - Local conditional distributions (factors) are the true conditional distributions.
  - $\underbrace{\mathbb{P}(D=d \mid A=a, B=b)}_{\text {from probabilistic inference }}=\underbrace{p(d \mid a, b)}_{\text {by definition }}$

### Summary so far

- Random variables capture state of world 
- Edges between variables represent dependencies 
- Local conditional distributions ⇒ joint distribution 
- Probabilistic inference: ask questions about world 
- Captures reasoning patterns (e.g., explaining away) 
- Factor graph interpretation (for inference later)

## **Probabilistic programs**

### e.g. Probabilistic program: alarm

- $B \sim \operatorname{Bernoulli}(\epsilon)$
- $E \sim \operatorname{Bernoulli}(\epsilon)$
- $A=B \vee E$

### Key idea: probabilistic program

- A randomized program that sets the random variables.

- ```python
  def Bernoulli(epsilon):
  	return random.random() < epsilon
  ```

### Summary so far

- Many many different types of models 
- Mindset: come up with stories of how the data (input) was generated through quantities of interest (output) 
- Opposite of how we normally do classification

## **Inference**

### Review: probabilistic inference

- Input
  - Bayesian network: $\mathbb{P}\left(X_{1}, \ldots, X_{n}\right)$
  - Evidence: $E=e$ where $E \subseteq X$ is subset of variables
  - Query: $Q \subseteq X$ is subset of variables
- Output
  - $\mathbb{P}(Q=q \mid E=e)$ for all values $q$

### General strategy

- Query: $\mathbb{P}(Q \mid E=e)$
- Algorithm: general probabilistic inference strategy
  - Remove (marginalize) variables that are not ancestors of $Q$ or $E$.
  - Convert Bayesian network to factor graph.
  - Condition on $E=e$ (shade nodes $+$ disconnect) .
  - Remove (marginalize) nodes disconnected from $Q$.
  - Run probabilistic inference algorithm (manual, variable elimination, Gibbs sampling, particle filtering).

## **Summary**

- Bayesian networks: modular definition of large joint distribution over variables
- Probabilistic inference: condition on evidence, query variables of interest
- Next time: algorithms for probabilistic inference

## *Thinking*

- 将贝叶斯推断和因子图结合，从而解决大量变量的推断问题
  - 本讲主要集中在概念的介绍和问题的建模

# **<u>L14. Forward-backward, Particle filtering, Gibbs sampling</u>**

### Knowledge about the world

- A Bayesian network specifies two parts: 
  - (i) a graph structure which governs the qualitative relationship between the variables, and 
  - (ii) local conditional distributions, which specify the quantitative relationship.
- Formally, a Bayesian network defines a joint probability distribution over many variables (e.g., $\mathbb{P}(C, A, H, I))$ via the local conditional distributions (e.g., $p(i \mid a)$ ). 
  - **<u>This joint distribution specifies all the information we know about how the world works. (while deep learning focus on one task)</u>**

## **Forward-backward**

### Hidden Markov model

- e.g.  object tracking
  - ​	<img src="image.assets/Screen Shot 2022-01-26 at 18.46.25.png" alt="Screen Shot 2022-01-26 at 18.46.25" style="zoom: 33%;" />
  - $H_{i} \in\{1, \ldots, K\}:$ location of object at time step $i$
  - $E_{i} \in\{1, \ldots, K\}$ : sensor reading at time step $i$
  - Start $p\left(h_{1}\right)$ : e.g., uniform over all locations
  - Transition $p\left(h_{i} \mid h_{i-1}\right)$ : e.g., uniform over adjacent loc.
  - Emission $p\left(e_{i} \mid h_{i}\right)$ : e.g., uniform over adjacent loc.
    $$
    \mathbb{P}(H=h, E=e)=\underbrace{p\left(h_{1}\right)}_{\text {start }} \prod_{i=2}^{n} \underbrace{p\left(h_{i} \mid h_{i-1}\right)}_{\text {transition }} \prod_{i=1}^{n} \underbrace{p\left(e_{i} \mid h_{i}\right)}_{\text {emission }}
    $$
- Hidden Markov model inference
  - Question (filtering):
    $$
    \mathbb{P}\left(H_{3} \mid E_{1}=e_{1}, E_{2}=e_{2}, E_{3}=e_{3}\right)
    $$
  - Question (smoothing):
    $$
    \mathbb{P}\left(H_{3} \mid E_{1}=e_{1}, E_{2}=e_{2}, E_{3}=e_{3}, E_{4}=e_{4}, E_{5}=e_{5}\right)
    $$

### Lattice representation

- ​	<img src="image.assets/Screen Shot 2022-01-26 at 18.48.37.png" alt="Screen Shot 2022-01-26 at 18.48.37" style="zoom:33%;" />
  - Edge start $\Rightarrow H_{1}=h_{1}$ has weight $p\left(h_{1}\right) p\left(e_{1} \mid h_{1}\right)$
  - Edge $H_{i-1}=h_{i-1} \Rightarrow H_{i}=h_{i}$ has weight $p\left(h_{i} \mid h_{i-1}\right) p\left(e_{i} \mid\right.$ $h_{i}$ )
  - Each path from start to end is an assignment with weight equal to the product of node/edge weights

### Forward-backward

- Forward: $F_{i}\left(h_{i}\right)=\sum_{h_{i-1}} F_{i-1}\left(h_{i-1}\right) w\left(h_{i-1}, h_{i}\right)$
  - sum of weights of paths from start to $H_{i}=h_{i}$
- Backward: $B_{i}\left(h_{i}\right)=\sum_{h_{i+1}} B_{i+1}\left(h_{i+1}\right) w\left(h_{i}, h_{i+1}\right)$
  - sum of weights of paths from $H_{i}=h_{i}$ to end
- Define $S_{i}\left(h_{i}\right)=F_{i}\left(h_{i}\right) B_{i}\left(h_{i}\right)$ :
  - sum of weights of paths from start to end through $H_{i}=h_{i}$
- Smoothing queries (marginals): $\mathbb{P}\left(H_{i}=h_{i} \mid E=e\right) \propto S_{i}\left(h_{i}\right)$
- Algorithm
  - Compute $F_{1}, F_{2}, \ldots, F_{n}$
  - Compute $B_{n}, B_{n-1}, \ldots, B_{1}$
  - Compute $S_{i}$ for each $i$ and normalize
- Running time: $O(nK^2)$

### Summary so far

- Lattice representation: paths are assignments (think state-based models) 
- Dynamic programming: compute sums efficiently 
- Forward-backward algorithm: share intermediate computations across different queries

## **Particle filtering**

### Review: beam search

- Idea: keep $\leq K$ candidate list $C$ of partial assignments
- Algorithm: beam search
  - Initialize $C \leftarrow[\{\}]$
  - For each $i=1, \ldots, n:$
    - Extend:
      - $C^{\prime} \leftarrow\left\{h \cup\left\{H_{i}: v\right\}: h \in C, v \in\right.$ Domain $\left._{i}\right\}$
    - Prune:
      - $C \leftarrow K$ elements of $C^{\prime}$ with highest weights
- Problem:
  - Extend: slow because requires considering every possible value for $H_{i}$
  - Prune: greedily taking best $K$ doesn't provide diversity

### Solution (3 steps): propose, weight, resample

- Step 1: propose (Key idea: proposal distribution)

  - For each old particle $\left(h_{1}, h_{2}\right)$, sample $H_{3} \sim p\left(h_{3} \mid h_{2}\right)$

- Step 2: weight (Key idea: weighting)

  - For each old particle $\left(h_{1}, h_{2}, h_{3}\right)$, weight it by $w\left(h_{1}, h_{2}, h_{3}\right)=p\left(e_{3} \mid h_{3}\right)$

- Step 3: resample

  - Tricky situation:

    - Target distribution close to uniform

    - Fewer particles than locations

  - Key idea: resampling

    - Given a distribution $\mathbb{P}(A=a)$ with $n$ possible values, draw a sample $K$ times.
    - Intuition: redistribute particles to more promising areas

### Particle filtering

- Algorithm
  - Initialize $C \leftarrow[\{\}]$
  - For each $i=1, \ldots, n:$
    - Propose (extend):
      - $C^{\prime} \leftarrow\left\{h \cup\left\{H_{i}: h_{i}\right\}: h \in C, h_{i} \sim p\left(h_{i} \mid h_{i-1}\right)\right\}$
    - Reweight:
      - Compute weights $w(h)=p\left(e_{i} \mid h_{i}\right)$ for $h \in C^{\prime}$
    - Resample (prune):
      - $C \leftarrow K$ elements drawn independently from $\propto w(h)$
- Implementation details
  - If only care about last $H_i$, collapse all particles with same $H_i$
  - If many particles are the same, can just store counts

## **Gibbs sampling**

### Setup: $\operatorname{Weight}(x)$

- Algorithm: Gibbs sampling

  - Initialize $x$ to a random complete assignment

  - Loop through $i=1, \ldots, n$ until convergence:
    - Compute weight of $x \cup\left\{X_{i}: v\right\}$ for each $v$
    - Choose $x \cup\left\{X_{i}: v\right\}$ with probability prop. to weight

### Setup: $\mathbb{P}(X=x) \propto$ Weight $(x)$

- Algorithm: Gibbs sampling (probabilistic interpretation)
  - Initialize $x$ to a random complete assignment 
  - Loop through $i=1, \ldots, n$ until convergence:
    - Set $X_{i}=v$ with prob. $\mathbb{P}\left(X_{i}=v \mid X_{-i}=x_{-i}\right)$
      ( $X_{-i}$ denotes all variables except $X_{i}$ )

## **Summary**

- Model (Bayesian network or factor graph):
  $$
  \mathbb{P}(X=x)=\prod_{i=1}^{n} p\left(x_{i} \mid x_{\text {Parents }(i)}\right)
  $$
- Probabilistic inference:
  $$
  \mathbb{P}(Q \mid E=e)
  $$
- Algorithms:
  - Forward-backward: HMMs, exact
  - Particle filtering: HMMs, approximate
  - Gibbs sampling: general, approximate

## *Thinking*

- 贝叶斯推断的算法介绍
  - 隐马尔可夫模型HMM
    - 前向-后向算法：类似动态规划的memoization
    - 粒子滤波：应对状态空间巨大的问题 -- 类比beam search，但展开时不需要遍历所有可能，而是通过随机采样来增加高权重样本的比例（也避免了beam search的贪婪）
    - 吉布斯采样：arbitrary factor graph，CSP中对应算法的概率解读（根据条件概率而非权重得出采样概率）

# **<u>L15. Learning Bayesian networks, Laplace smoothing, Expectation Maximization</u>**

## **Supervised learning**

### Learning task

- Definition
  - Training data
    - $\mathcal{D}_{\text {train }}($ an example is an assignment to $X)$
  
  - Parameters
    - $\theta$ (local conditional probabilities)
  
- Parameter sharing
  - The local conditional distributions of different variables use the same parameters.
  - Impact: more reliable estimates, less expressive model

### Example: HMMs

- Variables:
  - $H_{1}, \ldots, H_{n}$ (e.g., actual positions)
  - $E_{1}, \ldots, E_{n}$ (e.g., sensor readings)
  - $\mathbb{P}(H=h, E=e)=p_{\text {start }}\left(h_{1}\right) \prod_{i=2}^{n} p_{\text {trans }}\left(h_{i} \mid h_{i-1}\right) \prod_{i=1}^{n} p_{\text {emit }}\left(e_{i} \mid h_{i}\right)$
- Parameters: $\theta=\left(p_{\text {start }}, p_{\text {trans }}, p_{\text {emit }}\right)$
- $\mathcal{D}_{\text {train }}$ is a set of full assignments to $(H, E)$

### General case

- Bayesian network: variables $X_{1}, \ldots, X_{n}$
- Parameters: collection of distributions $\theta=\left\{p_{d}: d \in D\right\}$ (e.g., $D=$ \{start, trans, emit\})
- Each variable $X_{i}$ is generated from distribution $p_{d_{i}}$ :
  $$
  \mathbb{P}\left(X_{1}=x_{1}, \ldots, X_{n}=x_{n}\right)=\prod_{i=1}^{n} p_{d_{i}}\left(x_{i} \mid x_{\text {Parents }(i)}\right)
  $$
- Parameter sharing: $d_{i}$ could be same for multiple $i$

### Learning algorithm

- Input: training examples $\mathcal{D}_{\text {train }}$ of full assignments
- Output: parameters $\theta=\left\{p_{d}: d \in D\right\}$
- Algorithm: maximum likelihood for Bayesian networks
  - Count:
    - For each $x \in \mathcal{D}_{\text {train }}$ :
      - For each variable $x_{i}$ :
        - Increment count $_{d_{i}}\left(x_{\operatorname{Parents}(i)}, x_{i}\right)$
  - Normalize:
    - For each $d$ and local assignment $x_{\text {Parents }(i)}$ :
      - Set $p_{d}\left(x_{i} \mid x_{\text {Parents }(i)}\right) \propto \operatorname{count}_{d}\left(x_{\text {Parents }(i)}, x_{i}\right)$

### Maximum likelihood

- Maximum likelihood objective:
  - $\max _{\theta} \prod_{x \in \mathcal{D}_{\text {train }}} \mathbb{P}(X=x ; \theta)$
  - Previous learning algorithm exactly computes maximum likelihood parameters (closed form solution).

## **Laplace smoothing**

- When have less data, maximum likelihood overfits, want a more reasonable estimate...

### Regularization: Laplace smoothing

- Key idea: Laplace smoothing
  - For each distribution $d$ and partial assignment $\left(x_{\operatorname{Parents}(i)}, x_{i}\right)$, add $\lambda$ to count $_{d}\left(x_{\text {Parents }(i)}, x_{i}\right)$.
  - Then normalize to get probability estimates.
- Interpretation: hallucinate $\lambda$ occurrences of each local assignment
- Larger $\lambda \Rightarrow$ more smoothing $\Rightarrow$ probabilities closer to uniform.
- Data wins out in the end:

## **Unsupervised learning with EM**

- Motivation: What if we don’t observe some of the variables?

### Maximum marginal likelihood

- Variables: $H$ is hidden, $E = e$ is observed
- Maximum marginal likelihood objective:
  - $\max _{\theta} \prod_{e \in \mathcal{D}_{\text {train }}} \mathbb{P}(E=e ; \theta) \quad=\quad \max _{\theta} \prod_{e \in \mathcal{D}_{\text {train }}} \sum_{h} \mathbb{P}(H=h, E=e ; \theta)$

### Expectation Maximization (EM)

- Intuition: generalization of the K-means algorithm 
- Variables: $H$ is hidden, E = e is observed
- Algorithm: Expectation Maximization (EM)
  - Initialize $\theta$
  - Repeat until convergence:
    - E-step:
      - Compute $q(h)=\mathbb{P}(H=h \mid E=e ; \theta)$ for each $h$ (use any
        probabilistic inference algorithm)
      - Create weighted points: $(h, e)$ with weight $q(h)$
    - M-step:
      - Compute maximum likelihood (just count and normalize) to get $\theta$

### Application: decipherment as an HMM

- Strategy:
  - $p_{\text {start }}$ : set to uniform
  - $p_{\text {trans }}$ : estimate on tons of English text
  - $p_{\mathrm{emit}}$ : substitution table, from EM
  - Intuition: rely on language model $\left(p_{\text {trans }}\right)$ to favor plaintexts $h$ that look like English
- E-step: forward-backward algorithm computes
  - $q_{i}(h) \stackrel{\text { def }}{=} \mathbb{P}\left(H_{i}=h \mid E_{1}=e_{1}, \ldots E_{n}=e_{n}\right)$
- M-step: count (fractional) and normalize
  - $\operatorname{count}_{\text {emit }}(h, e)=\sum_{i=1}^{n} q_{i}(h) \cdot\left[e_{i}=e\right]$
  -  $p_{\text {emit }}(e \mid h) \propto$ count $_{\text {emit }}(h, e)$

## **Summary**

- (Bayesian network without parameters) + training examples
- Learning: maximum likelihood (+Laplace smoothing, +EM)
- $Q \mid E \Rightarrow \quad \begin{gathered}\text { Parameters } \theta \\ \text { (of Bayesian network) }\end{gathered} \quad \Rightarrow \mathbb{P}(Q \mid E ; \theta)$

## *Thinking*

- 通过数据学习贝叶斯网络的局部条件概率
  - 完整数据：Count and Normalize - Maximum Likelihood Estimation
  - 正则化：拉普拉斯平滑 - Additive smoothing
  - 部分数据：最大期望算法 - Expectation and Maximization































