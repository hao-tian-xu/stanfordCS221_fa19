# **<u>L9. Minimax, expectimax, Evaluation functions, Alpha-beta pruning</u>**

## **Games, expectimax**

### Game tree

- Each node is a decision point for a player. 
- Each root-to-leaf path is a possible outcome of the game.

### Two-player zero-sum games

- $\text{Players} = \{\text{agent, opp}\}$
- Turn taking
- Definition
  - $s_{\text {start }}$ starting state
  - $\operatorname{Actions}(s)$ : possible actions from state $s$
  - $\operatorname{Succ}(s, a)$ : resulting state if choose action $a$ in state $s$
  - $\operatorname{IsEnd}(s):$ whether $s$ is an end state (game over)
  - $\operatorname{Utility}(s):$ agent's utility for end state $s$
  - $\operatorname{Player}(s) \in$ Players: player who controls state $s$
- e.g. chess
  - $\text{Players}=\{\text{white, black}\}$
  - $\operatorname{State}s$ : (position of all pieces, whose turn it is)
  - $\operatorname{Actions}(s)$ : legal chess moves that Player $(s)$ can make
  - $\operatorname{IsEnd}(s)$ : whether $s$ is checkmate or draw
  - $\operatorname{Utility}(s):+\infty$ if white wins, 0 if draw, $-\infty$ if black wins

### Characteristics of games

- All the utility is at the end state
- Different players in control at different states

### Policies

- Deterministic policies: $\pi_{p}(s) \in \operatorname{Actions}(s)$
  - action that player $p$ takes in state $s$
- Stochastic policies $\pi_{p}(s, a) \in[0,1]$ :
  - probability of player $p$ taking action $a$ in state $s$

### Game evaluation recurrence

- Analogy: recurrence for policy evaluation in MDPs

- Given $\pi_{\text{agent}}\text{ and } \pi_{\text{opp}}$

- Value of the game
	$$
  V_{\text {eval }}(s)= \begin{cases}\text { Utility }(s) & \operatorname{IsEnd}(s) \\ \sum_{a \in \text { Actions }(s)} & \pi_{\text {agent }}(s, a) V_{\text {eval }}(\operatorname{Succ}(s, a)) \\ \sum_{a \in \text { Actions }(s)} & \pi_{\text {opp }}(s, a) V_{\text {eval }}(\operatorname{Succ}(s, a)) \quad \operatorname{Player}(s)=\operatorname{agent}\end{cases}
  $$

### Expectimax recurrence

- Analogy: recurrence for value iteration in MDPs
- Only given $\pi_{\text{opp}}$
- Value of the game
	$$
  V_{\text {exptmax }}(s)= \begin{cases}\text { Utility }(s) & \text { IsEnd }(s) \\ \max _{a \in \operatorname{Actions}(s)} V_{\text {exptmax }}(\operatorname{Succ}(s, a)) & \text { Player }(s)=\text { agent } \\ \sum_{a \in \text { Actions }(s)} \pi_{\text {opp }}(s, a) V_{\text {exptmax }}(\operatorname{Succ}(s, a)) & \text { Player }(s)=\text { opp }\end{cases}
  $$

## **Minimax, expetiminimax**

- Problem: don’t know opponent’s policy 
- Approach: assume the worst case

### Minimax

$$
V_{\operatorname{minmax}}(s)= \begin{cases}\operatorname{Utility}(s) & \operatorname{IsEnd}(s) \\ \max _{a \in \operatorname{Actions}(s)} V_{\operatorname{minmax}}(\operatorname{Succ}(s, a)) & \operatorname{Player}(s)=\operatorname{agent} \\ \min _{a \in \operatorname{Actions}(s)} V_{\min \max }(\operatorname{Succ}(s, a)) & \operatorname{Player}(s)=\mathrm{opp}\end{cases}
$$

### Extracting minimax policies

$$
\begin{aligned}
&\pi_{\max }(s)=\arg \max _{a \in \operatorname{Actions}(s)} V_{\operatorname{minmax}}(\operatorname{Succ}(s, a)) \\
&\pi_{\min }(s)=\arg \min _{a \in \text { Actions }(s)} V_{\operatorname{minmax}}(\operatorname{Succ}(s, a))
\end{aligned}
$$

### Relationship between game values

$$
\begin{array}{ccc} 
& \pi_{\min } && \pi_{7} \\
\pi_{\max } & V\left(\pi_{\max }, \pi_{\min }\right) & \leq & V\left(\pi_{\max }, \pi_{7}\right) \\
& \geq && \leq \\
\pi_{\operatorname{exptmax}(7)}& V\left(\pi_{\operatorname{exptmax}(7)}, \pi_{\min }\right) & & V\left(\pi_{\operatorname{exptmax}(7)}, \pi_{7}\right)
\end{array}
$$

###  Expectiminimax example

- see slides
- main idea: rebuid the game tree with three players

$$
V_{\text {exptminmax }}(s)= \begin{cases}\text { Utility }(s) & \text { IsEnd }(s) \\ \max _{a \in \operatorname{Actions}(s)} V_{\text {exptminmax }}(\operatorname{Succ}(s, a)) & \operatorname{Player}(s)=\operatorname{agent} \\ \min _{a \in \operatorname{Actions}(s)} V_{\text {exptminmax }}(\operatorname{Succ}(s, a)) & \operatorname{Player}(s)=\text { opp } \\ \sum_{a \in \operatorname{Actions}(s)} \pi_{\text {coin }}(s, a) V_{\text {exptminmax }}(\operatorname{Succ}(s, a)) & \operatorname{Player}(s)=\operatorname{coin}\end{cases}
$$

### Summary so far

- Primitives: max nodes, chance nodes, min nodes 
- Composition: alternate nodes according to model of game 
- Value function V···(s): recurrence for expected utility Scenarios to think about: 
  - What if you are playing against multiple opponents? 
  - What if you and your partner have to take turns (table tennis)? 
  - Some actions allow you to take an extra turn?

### Complexity

- branching factor $b$, depth $d$ (2d plies)
- $O(d)$ space, $O\left(b^{2 d}\right)$ time

## **Evaluation functions**

- use domain-specific knowledge, compute approximate answer

### Depth-limited search

- Limited depth tree search (stop at maximum depth $d_{\max }$ ):
  $$
  V_{\text {minmax }}(s, d)= \begin{cases}\text { Utility }(s) & \operatorname{lsEnd}(s) \\ \operatorname{Eval}(s) & d=0 \\ \max _{a \in \operatorname{Actions}(s)} V_{\operatorname{minmax}}(\operatorname{Succ}(s, a), d) & \operatorname{Player}(s)=\operatorname{agent} \\ \min _{a \in \operatorname{Actinns}(s)} V_{\min m a x}(\operatorname{Succ}(s, a), d-1) & \operatorname{Player}(s)=\mathrm{opp}\end{cases}
  $$
- Use: at state $s$, call $V_{\operatorname{minmax}}\left(s, d_{\max }\right)$
- Convention: decrement depth at last player's turn

### Evaluation functions

- Definition
  - An evaluation function $\operatorname{Eval}(s)$ is a (possibly very weak) estimate of the value $V_{\operatorname{minmax}}(s)$.
- Analogy: $\operatorname{FutureCost}(s)$ in search problems
- e.g. chess
  - $\text { Eval }(s)=\text { material }+\text { mobility }+\text { king-safety }+\text { center-control }$
    - $\text { material }=10^{100}\left(K-K^{\prime}\right)+9\left(Q-Q^{\prime}\right)+5\left(R-R^{\prime}\right)+\quad 3\left(B-B^{\prime}+N-N^{\prime}\right)+1\left(P-P^{\prime}\right)$
    - $\text { mobility }=0.1 (\text {num-legal-moves - num-legal-moves}^{\prime})$

### Summary: evaluation functions

- Eval $(s)$ attempts to estimate $V_{\operatorname{minmax}}(s)$ using domain knowledge
- No guarantees (unlike $A^{*}$ ) on the error from approximation

## **Alpha-beta pruning**

- Key idea: branch and bound
  - Maintain lower and upper bounds on values. 
  - If intervals don’t overlap non-trivially, then can choose optimally without further work
- Key idea: optimal path
  - The optimal path is path that minimax policies take. 
  - Values of all nodes on path are the same.
- Steps
  - $a_{s}$ : lower bound on value of $\max$ node $s$
  - $b_{s}$ : upper bound on value of $\min$ node $s$
  - Prune a node if its interval doesn't have non-trivial overlap with every ancestor (store $\alpha_{s}=\max _{s^{\prime} \preceq s} a_{s}$ and $\beta_{s}=$ $\left.\min _{s^{\prime} \preceq s} b_{s}\right)$

### Move ordering

- Which ordering to choose?
  - Worst ordering: $O\left(b^{2 \cdot d}\right)$ time
  - Best ordering: $O\left(b^{2 \cdot 0.5 d}\right)$ time
  - Random ordering: $O\left(b^{2 \cdot 0.75 d}\right)$ time
- In practice, can use evaluation function $\operatorname{Eval}(s)$ :
  - Max nodes: order successors by decreasing $\operatorname{Eval}(s)$
  - Min nodes: order successors by increasing $\operatorname{Eval}(s)$

## **Summary**

- Game trees: model opponents, randomness
- Minimax: find optimal policy against an adversary
- Evaluation functions: domain-specific, approximate
- Alpha-beta pruning: domain-general, exact

## *Thinking*

- 如果说MDP是自然的规律作为对手，Game中则是一个有形的对手，有没有关于对手的知识是有很大差别的
  - 实际上将对手建模后即可以理解为MDP的问题
    - 对手可以为有固定随机策略的，也可以为最小化玩家收益的
- 由于Game的深度和广度可能都很大，因此需要加速的策略
  - 通过对Game的知识得到价值函数，进而在一定深度后预测该状态的价值
  - 通过对状态价值的上下限的比较，剔除一些状态的探索



# **<u>L10. TD learning, Game theory</u>**

## **Temporal difference learning**

- Evaluation function
  - Old: hand-crafted
  - New: learn from data

### Model for evaluation functions

- Linear:
  $$
  V(s ; \mathbf{w})=\mathbf{w} \cdot \phi(s)
  $$
  
- Neural network:
  $$
  V\left(s ; \mathbf{w}, \mathbf{v}_{1: k}\right)=\sum_{j=1}^{k} w_{j} \sigma\left(\mathbf{v}_{j} \cdot \phi(s)\right)
  $$

### Generating data

- Generate using policies based on current $V(s ; \mathbf{w})$ :
  $$
  \begin{aligned}
  &\pi_{\text {agent }}(s ; \mathbf{w})=\arg \max _{a \in \operatorname{Actions}(s)} V(\operatorname{Succ}(s, a) ; \mathbf{w}) \\
  &\pi_{\mathrm{opp}}(s ; \mathbf{w})=\arg \min _{a \in \operatorname{Actions}(s)} V(\operatorname{Succ}(s, a) ; \mathbf{w})
  \end{aligned}
  $$
  Note: don't need to randomize $(\epsilon$-greedy $)$ because game is already stochastic (backgammon has dice) and there's function approximation

### Learning algorithm

- A small piece of experience:
  $$
  \left(s, a, r, s^{\prime}\right)
  $$
- Prediction:
  $$
  V(s ; \mathbf{w})
  $$
- Target:
  $$
  r+\gamma V\left(s^{\prime} ; \mathbf{w}\right)
  $$

### General framework

- Objective function:
  $$
  \frac{1}{2}(\text { prediction }(\mathbf{w}) \text { - target })^{2}
  $$
- Gradient:
  $$
  \text { (prediction( } \mathbf{w} \text { ) - target) } \nabla_{\mathbf{w}} \text { prediction( } \mathbf{w} \text { ) }
  $$
- Update:
  $$
  \mathbf{w} \leftarrow \mathbf{w}-\eta \underbrace{(\text { prediction }(\mathbf{w})-\text { target }) \nabla_{\mathbf{w}} \text { prediction }(\mathbf{w})}_{\text {gradient }}
  $$

### Algorithm: TD learning

- On each $\left(s, a, r, s^{\prime}\right)$ :
  $$
  \mathbf{w} \leftarrow \mathbf{w}-\eta[\underbrace{V(s ; \mathbf{w})}_{\text {prediction }}-\underbrace{\left(r+\gamma V\left(s^{\prime} ; \mathbf{w}\right)\right)}_{\text {target }}] \nabla_{\mathbf{w}} V(s ; \mathbf{w})
  $$
- For linear functions:
  $$
  \begin{aligned}
  &V(s ; \mathbf{w})=\mathbf{w} \cdot \phi(s) \\
  &\nabla_{\mathbf{w}} V(s ; \mathbf{w})=\phi(s)
  \end{aligned}
  $$

### Comparison

- Q-learning:
  - Operate on $\hat{Q}_{\mathrm{opt}}(s, a ; \mathbf{w})$
  - Off-policy: value is based on estimate of optimal policy
  - To use, don't need to know MDP transitions $T\left(s, a, s^{\prime}\right)$
- TD learning:
  - Operate on $\hat{V}_{\pi}(s ; \mathbf{w})$
  - On-policy: value is based on exploration policy (usually based on $\left.\hat{V}_{\pi}\right)$
  - To use, need to know rules of the game $\operatorname{Succ}(s, a)$

### Examples

- Arthur Samuel's checkers program [1959]:
  - Learned by playing itself repeatedly (self-play)
  - Smart features, linear evaluation function, use intermediate rewards
  - Used alpha-beta pruning $+$ search heuristics
  - Reach human amateur level of play
  - IBM 701: 9K of memory!
- Gerald Tesauro's TD-Gammon [1992]:
  - Learned weights by playing itself repeatedly (1 million times)
  - Dumb features, neural network, no intermediate rewards
  - Reached human expert level of play, provided new insights into opening
- AlphaGo Zero [2017]:
  - Learned by self play (4.9 million games)
  - Dumb features (stone positions), neural network, no intermediate rewards, Monte Carlo Tree Search
  - Beat AlphaGo, which beat Le Sedol in 2016
  - Provided new insights into the game

## **Simultaneous games**

### Payoff matrix

- Definition: single-move simultaneous game
  - Players $=\{\mathrm{A}, \mathrm{B}\}$
  - Actions: possible actions
  - $V(a, b):$ A's utility if A chooses action $a$, B chooses $b$
    (let $V$ be payoff matrix)

### Strategies (policies)

- Definition: pure strategy
  - A pure strategy is a single action:
    $a \in$ Actions
- Definition: mixed strategy
  - A mixed strategy is a probability distribution
    $0 \leq \pi(a) \leq 1$ for $a \in$ Actions

### Game evaluation

- Definition: game evaluation

  - The value of the game if player $A$ follows $\pi_{A}$ and player $B$ follows $\pi_{\mathrm{B}}$ is
    $$
    V\left(\pi_{\mathrm{A}}, \pi_{\mathrm{B}}\right)=\sum_{a, b} \pi_{\mathrm{A}}(a) \pi_{B}(b) V(a, b)
    $$

### Pure strategies

- Proposition: going second is no worse
  - $\max _{a} \min _{b} V(a, b) \leq \min _{b} \max _{a} V(a, b)$

### Mixed strategies

- Proposition: second player can play pure strategy

  - For any fixed mixed strategy $\pi_{A}$ :
    $$
    \min _{\pi_{\mathrm{B}}} V\left(\pi_{\mathrm{A}}, \pi_{B}\right)
    $$
    can be attained by a pure strategy.
- Theorem: minimax theorem [von Neumann, 1928]
  - For every simultaneous two-player zero-sum game with a finite number of actions:
    $$
    \max _{\pi_{\mathrm{A}}} \min _{\pi_{\mathrm{B}}} V\left(\pi_{\mathrm{A}}, \pi_{\mathrm{B}}\right)=\min _{\pi_{\mathrm{B}}} \max _{\pi_{\mathrm{A}}} V\left(\pi_{\mathrm{A}}, \pi_{\mathrm{B}}\right),
    $$
    where $\pi_{A}, \pi_{B}$ range over mixed strategies.

  - Upshot: revealing your optimal mixed strategy doesn't hurt you!

## **Non-zero-sum games**

- Competitive games: minimax (linear programming)
- Collaborative games: pure maximization (plain search)
- Real life: ?

### Prisoner’s dilemma

- Prosecutor asks A and B individually if each will testify against the other. 
- If both testify, then both are sentenced to 5 years in jail. 
- If both refuse, then both are sentenced to 1 year in jail. 
- If only one testifies, then he/she gets out for free; the other gets a 10-year sentence.

### Payoff matrix

| B \ A       | testify        | refuse         |
| ----------- | -------------- | -------------- |
| **testify** | A = -5, B = -5 | A = -10, B = 0 |
| **refuse**  | A = 0, B = -10 | A = -1, B = -1 |

### Nash equilibrium

- Definition

  - A Nash equilibrium is $\left(\pi_{\mathrm{A}}^{*}, \pi_{\mathrm{B}}^{*}\right)$ such that no player has an incentive to change his/her strategy:
    $$
    \begin{aligned}
    &V_{\mathrm{A}}\left(\pi_{\mathrm{A}}^{*}, \pi_{B}^{*}\right) \geq V_{\mathrm{A}}\left(\pi_{\mathrm{A}}, \pi_{\mathrm{B}}^{*}\right) \text { for all } \pi_{A} \\
    &V_{\mathrm{B}}\left(\pi_{\mathrm{A}}^{*}, \pi_{B}^{*}\right) \geq V_{\mathrm{B}}\left(\pi_{\mathrm{A}}^{*}, \pi_{\mathrm{B}}\right) \text { for all } \pi_{B}
    \end{aligned}
    $$

- Theorem: Nash’s existence theorem [1950]

  - In any finite-player game with finite number of actions, there exists at least one Nash equilibrium.

## **Summary**

- Main challenge: not just one objective 
- Minimax principle: guard against adversary in turn-based games 
- Simultaneous non-zero-sum games: mixed strategies, Nash equilibria 
- Strategy: search game tree + learned evaluation function

## *Thinking*

- 时序差分学习（TD Learning）
  - 也是通过梯度下降来实现权重向量的学习
  - 需要特征向量的提取（深度学习可以简化这一步）
- 游戏理论
  - 即时制游戏：混合策略下的极小化极大定理（冯·诺依曼定理）
  - 非零和游戏：囚徒困境，纳什均衡



# **<u>Takeaway from L11.</u>**

## **State-based models**

| <u>Modeling</u>      |                       |                          |
| -------------------- | --------------------- | ------------------------ |
| **Framework**        | search problems       | MDPs / games             |
| **Objective**        | minimum cost paths    | maximum value policies   |
| **<u>Inference</u>** |                       |                          |
| **Tree-based**       | backtracking          | minimax / expectimax     |
| **Graph-based**      | DP, UCS, A*           | value / policy iteration |
| **<u>Learning</u>**  |                       |                          |
| **Methods**          | structured Perceptron | Q-learning, TD learning  |



















