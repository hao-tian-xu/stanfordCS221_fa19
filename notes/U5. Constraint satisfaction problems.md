# **<u>L11. Factor graphs, Backtracking search, Dynamic ordering, arc consistency</u>**

### State-based models: takeaway

- Key idea: specify locally, optimize globally
  - Modeling: specifies local interactions
  - Inference: find globally optimal solutions
- Key idea: state
  - A state is a summary of all the past actions sufficient to choose future actions optimally.
  - Mindset: move through states (nodes) via actions (edges)

### Variable-based modeling

- Problem
  - Variable ordering doesn’t affect correctness 
  - Variables are interdependent in a local way
- Key idea: variables
  - Solutions to problems ⇒ assignments to variables (modeling).
  - Decisions about variable ordering, etc. chosen by inference.

## **Factor graphs**

### Factor graphs

- Definition: factor graph
  - **Variables**:
    $X=\left(X_{1}, \ldots, X_{n}\right)$, where $X_{i} \in$ Domain $_{i}$
  - **Factors**:
    $f_{1}, \ldots, f_{m}$, with each $f_{j}(X) \geq 0$
- Definition: scope and arity
  - **Scope** of a factor $f_{j}$ : set of variables it depends on. 
  - **Arity** of $f_{j}$ is the number of variables in the scope. 
  - **Unary** factors (arity 1); **Binary** factors (arity 2).
- Definition: assignment weight
  - Each **assignment** $x=\left(x_{1}, \ldots, x_{n}\right)$ has a **weight**:
    $$
    \text { Weight }(x)=\prod_{j=1}^{m} f_{j}(x)
    $$
  - Objective: find the maximum weight assignment
    $$
    \arg \max _{x} \operatorname{Weight}(x)
    $$

### Constraint satisfaction problems

- Definition: constraint satisfaction problem (CSP)

  - A CSP is a factor graph where all factors are **constraints**:
    $$
    f_{j}(x) \in\{0,1\} \text { for all } j=1, \ldots, m
    $$
    The constraint is satisfied iff $f_{j}(x)=1$.

- Definition: consistent assignments

  - An assignment $x$ is consistent iff Weight $(x)=1$ (i.e., all constraints are satisfied).

### Factor Graph vs. CSP

| Factor graph (general) | CSP (all or nothing)       |
| ---------------------- | -------------------------- |
| variables              | variables                  |
| factors                | constraints                |
| assignment weight      | consistent or inconsistent |

## **Dynamic ordering**

### Dependent factors

- Definition: dependent factors
  - Let $D\left(x, X_{i}\right)$ be set of factors depending on $X_{i}$ and $x$ but not on unassigned variables.

### Backtracking search

- Backtrack ($x, w, \text{Domains}$):
  - If $x$ is complete assignment: update best and return
  - Choose unassigned **VARIABLE** $X_{i}$ via **MOST CONSTRAINED VARIABLE**
  - Order **VALUES** $\text{Domain}_i$ of chosen $X_{i}$ via **LEAST CONSTRAINED VALUE**
  - For each value $v$ in that order:
    - $\delta \leftarrow \prod_{f_{j} \in D\left(x, X_{i}\right)} f_{j}\left(x \cup\left\{X_{i}: v\right\}\right)$
    - If $\delta=0$ : continue
    - $\text{Domains'} \leftarrow \text{Domains}$ via **LOOKAHEAD** (Forward checking or Arc consistency)
    - Backtrack $\left(x \cup\left\{X_{i}: v\right\}, w \delta, \text{Domains'}\right)$

### Lookahead: forward checking

- Key idea: forward checking (one-step lookahead)
  - After assigning a variable $X_{i}$, eliminate inconsistent values from the domains of $X_{i}$ 's neighbors.
  - If any domain becomes empty, don't recurse.
  - When unassign $X_{i}$, restore neighbors' domains.
- Need to actually prune domains to make heuristics useful

### Heuristic:choosing an unassigned variable

- Key idea: most constrained variable (MCV)
  - Choose variable that has the fewest consistent values.
- Intuition
  - Must assign every variable
  - If going to fail, fail early ⇒ more pruning
- Useful when some factors are constraints (can prune assignments with weight 0)

### Heuristic: order values of a selected variable

- Key idea: least constrained value (LCV)
  - Order values of selected $X_{i}$ by decreasing number of consistent values of neighboring variables.
- Intuition
  - Need to choose some value
  - Choosing value most likely to lead to solution
- Useful when all factors are constraints (all assignment weights are 1 or 0)

## **Arc consistency**

- Idea: eliminate values from domains ⇒ reduce branching
- Definition: arc consistency
  - A variable $X_{i}$ is arc consistent with respect to $X_{j}$ if for each $x_{i} \in \text{Domain}_{i}$, there exists $x_{j} \in\text{Domain}_{j}$ such that $f\left(\left\{X_{i}: x_{i}, X_{j}:\right.\right.$ $\left.\left.x_{j}\right\}\right) \neq 0$ for all factors $f$ whose scope contains $X_{i}$ and $X_{j}$.

### AC-3

- Forward checking: when assign $X_{j}: x_{j}$, set Domain $_{j}=\left\{x_{j}\right\}$ and enforce arc consistency on all neighbors $X_{i}$ with respect to $X_{j}$
- AC-3: repeatedly enforce arc consistency on all variables
- Algorithm
  - Add $X_{j}$ to set.
  - While set is non-empty:
    - Remove any $X_{k}$ from set.
    - For all neighbors $X_{l}$ of $X_{k}$ :
      - Enforce arc consistency on $X_{l}$ w.r.t. $X_{k}$.
      - If Domain $_{l}$ changed, add $X_{l}$ to set.

## Summary

- Basic template: backtracking search on partial assignments 
- Dynamic ordering: most constrained variable (fail early), least constrained value (try to succeed) 
- Lookahead: forward checking (enforces arc consistency on neighbors), AC-3 (enforces arc consistency on neighbors and their neighbors, etc.)

## **Modeling (examples)**

- <u>TODO</u>

## **Summary**

- Factor graphs: modeling framework (variables, factors) 
- Key property: ordering decisions pushed to algorithms 
- Algorithms: backtracking search + dynamic ordering + lookahead 
- Modeling: lots of possibilities!

## *Thinking*

- Factor Graph is another way of modeling the search problem, mainly for search problems that are not sensitive to sequential problems
  - The decision of the order is included in the algorithm
  - The result is the assignment of all variables



- 因子图Factor Graph是对于搜索问题的另一种建模方式，主要针对对于顺序问题不敏感的搜索问题
  - 顺序的决定包含在算法中
  - 其结果为所有变量的赋值




# **<u>L12. Beam search, local search, Conditional independence, variable elimination</u>**

## **Beam search**

- Backtracking search: $O(b^n)$

### Greedy search

- Algorithm
  - Partial assignment $x \leftarrow\{\}$
  - For each $i=1, \ldots, n:$
    - Extend:
      - Compute weight of each $x_{v}=x \cup\left\{X_{i}: v\right\}$
    - Prune:
      - $x \leftarrow x_{v}$ with highest weight
- Not guaranteed to find optimal assignment!

### Beam search

- Idea: keep $\leq K$ candidate list $C$ of partial assignments
- Algorithm
  - Initialize $C \leftarrow[\{\}]$
  - For each $i=1, \ldots, n$ :
    - Extend:
      - $C^{\prime} \leftarrow\left\{x \cup\left\{X_{i}: v\right\}: x \in C, v \in \operatorname{Domain}_{i}\right\}$
    - Prune:
      - $C \leftarrow K$ elements of $C^{\prime}$ with highest weights
- Not guaranteed to find optimal assignment
- Properties
  - Running time: $O(n(K b) \log (K b))$ with branching factor $b=$ |Domain|, beam size $K$
  - Beam size $K$ controls tradeoff between efficiency and accuracy
    - $K=1$ is greedy ( $O(n b)$ time)
    - $K=\infty$ is BFS tree search $\left(O\left(b^{n}\right)\right.$ time $)$
  - Analogy: backtracking search : DFS :: BFS : beam search (pruned)

## **Local search**

### Iterated conditional modes (ICM)

- Key idea: locality
  - When evaluating possible re-assignments to $X_i$ , only need to consider the factors that depend on $X_i$ .
- Algorithm
  - Initialize $x$ to a random complete assignment 
  - Loop through $i=1, \ldots, n$ until convergence: 
    - Compute weight of $x_{v}=x \cup\left\{X_{i}: v\right\}$ for each $v$ $x \leftarrow x_{v}$ with highest weight
- Properties
  - Weight(x) increases or stays the same each iteration 
  - Converges in a finite number of iterations 
  - Can get stuck in local optima 
  - Not guaranteed to find optimal assignment!

### Gibbs sampling

- Key idea: randomness
  - Sometimes, need to go downhill to go uphill...
  - Sample an assignment with probability proportional to its weight.
- Algorithm
  - Initialize $x$ to a random complete assignment
  - Loop through $i=1, \ldots, n$ until convergence:
    - Compute weight of $x_{v}=x \cup\left\{X_{i}: v\right\}$ for each $v$
    - Choose $x \leftarrow x_{v}$ with probability prop. to its weight
- Can escape from local optima (not always easy though)

## Summary so far

- Algorithms for max-weight assignments in factor graphs: 
  - Extend partial assignments: 
    - Backtracking search: exact, exponential time 
    - Beam search: approximate, linear time
  - Modify complete assignments: 
    - Iterated conditional modes: approximate, deterministic 
    - Gibbs sampling: approximate, randomized

## **Conditioning**

- Motivation
  - Leverage graph properties to derive efficient algorithms which are exact.
  - Backtracking search: exponential time in number of variables $n$
  - Efficient algorithm: maximize each variable separately

### Independence

- Definition
  - Let $A$ and $B$ be a partitioning of variables $X$.
  - We say $A$ and $B$ are independent if there are no edges between $A$ and $B$.
  - In symbols: $A \Perp B$.

### Conditioning

- Definition
  - To condition on a variable $X_{i}=v$, consider all factors $f_{1}, \ldots, f_{k}$ that depend on $X_{i}$.
  - Remove $X_{i}$ and $f_{1}, \ldots, f_{k}$.
  - Add $g_{j}(x)=f_{j}\left(x \cup\left\{X_{i}: v\right\}\right)$ for $j=1, \ldots, k$.

### Conditional independence

- Definition
  - Let $A, B, C$ be a partitioning of the variables.
  - We say $A$ and $B$ are conditionally independent given $C$ if conditioning on $C$ produces a graph in which $A$ and $B$ are independent.
  - In symbols: $A \Perp B \mid C$.
- Equivalently: every path from $A$ to $B$ goes through $C$.

### Markov blanket

- Definition
  - Let $A \subseteq X$ be a subset of variables.
  - Define MarkovBlanket $(A)$ be the neighbors of $A$ that are not in $A$.
- Proposition: conditional independence
  - Let $C=$ MarkovBlanket $(A)$.
  - Let $B$ be $X \backslash(A \cup C)$.
  - Then $A \Perp B \mid C$.

### Summary so far

- Independence: when sets of variables A and B are disconnected; can solve separately. 
- Conditioning: assign variable to value, replaces binary factors with unary factors 
- Conditional independence: when C blocks paths between A and B 
- Markov blanket: what to condition on to make A conditionally independent of the rest.

## **Elimination**

### Conditioning versus elimination

- Conditioning:
  - Removes $X_{i}$ from the graph
  - Add factors that use fixed value of $X_{i}$
- Elimination (max):
  - Removes $X_{i}$ from the graph
  - Add factors that maximize over all values of $X_{i}$

### Elimination

- Definition
  - To eliminate a variable $X_{i}$, consider all factors $f_{1}, \ldots, f_{k}$ that depend on $X_{i}$.
  - Remove $X_{i}$ and $f_{1}, \ldots, f_{k}$.
  - Add $f_{\text {new }}(x)=\max _{x_{i}} \prod_{j=1}^{k} f_{j}(x)$
- Solves a mini-problem over $X_{i}$ conditioned on its Markov blanket
- Scope of $f_{\text {new }}$ is MarkovBlanket $\left(X_{i}\right)$

### Variable elimination algorithm

- Algorithm
  - For $i=1, \ldots, n$ :
    - Eliminate $X_{i}$ (produces new factor $f_{\text {new }, i}$ ).
  - For $i=n, \ldots, 1$ :
    - Set $X_{i}$ to the maximizing value in $f_{\text {new }, i}$.
- Let max-arity be the maximum arity of any $f_{\text {new }, i}$.
  Running time: $O(n \cdot \left| \text{Domain} \right|^{\text {max-arity }+1})$
- Variable ordering
  - Degree heuristic: eliminate variables with the fewest neighbors
- Treewidth
  - Definition: The treewidth of a factor graph is the maximum arity of any factor created by variable elimination with the best variable ordering.
  - e.g.
    - Treewidth of a chain is 1. 
    - Treewidth of a tree is 1. 
    - Treewidth of simple cycle is 2. 
    - Treewidth of m × n grid is min(m, n).

## **Summary**

- Beam search: follows the most promising branches of search tree based on myopic information (think pruned BFS search) 
- Local search: can freely re-assign variables; use randomness to get out of local optima 
- Conditioning: break up a factor graph into smaller pieces (divide and conquer); can use in backtracking 
- Elimination: solve a small subproblem conditioned on its Markov blanket

## *Thinking*

- More algorithms for factor graphs are introduced, on the one hand to reduce the complexity and get as close to the optimal solution as possible, and on the other hand for different interpretations of factor graphs



- 介绍了更多factor graph的算法，一方面在于减少复杂度并尽可能接近最优解，另一方面在于对factor graph的不同解读



### Aux: LaTex new command

- independence: $\newcommand{\Perp}{\perp\!\!\!\!\perp} \Perp$
